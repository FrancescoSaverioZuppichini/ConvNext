{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing ConvNext in PyTorch\n",
    "\n",
    "Hello There!! Today we are going to implement the famous ConvNext in PyTorch proposed in [A ConvNet for the 2020s\n",
    "](https://arxiv.org/abs/2201.03545).\n",
    "\n",
    "Code is here, an interactive version of this article can be downloaded from here.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "The paper proposes a new convolution-based architecture that not only surpasses Transformer-based model (such as Swin) but also scales with the amount of data! The following pictures show ConvNext accuracy against the different datasets/models sizes.\n",
    "\n",
    "\n",
    "<img src=\"./images/accuracy_table.png\" width=\"500px\"></img>\n",
    "\n",
    "\n",
    "So the authors started by taking the well know ResNet architecture and iteratively improving it following new best practices and discoveries made in the last decade. The authors focused on Swin-Transformer and follows closely its design choices.  The paper is top-notch, I highly recommend read it :) \n",
    "\n",
    "The following image shows all the various improvements and the respective performance after each one of them. \n",
    "\n",
    "<img src=\"./images/convnext_improvements.png\" width=\"500px\"></img>\n",
    "\n",
    "They divided their roadmap into two parts: macro design and micro design. Macro design is all the changes we do from a high-level perspective, e.g. the number of stages, while micro design is more about smaller things, e.g. which activation to use.\n",
    "\n",
    "We will now start with a classic BottleNeck block and apply each change one after the one.\n",
    "\n",
    "### Starting point: ResNet\n",
    "\n",
    "As you know (if you don't I have an [article about implementing ResNet in PyTorch](https://medium.com/p/a7da63c7b278)) ResNet uses a residual BottleNeck block, this will be our starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from typing import List\n",
    "\n",
    "class ConvNormAct(nn.Sequential):\n",
    "    \"\"\"\n",
    "    A little util layer composed by (conv) -> (norm) -> (act) layers.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        kernel_size: int,\n",
    "        norm = nn.BatchNorm2d,\n",
    "        act = nn.ReLU,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(\n",
    "                in_features,\n",
    "                out_features,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2,\n",
    "                **kwargs\n",
    "            ),\n",
    "            norm(out_features),\n",
    "            act(),\n",
    "        )\n",
    "\n",
    "class BottleNeckBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        reduction: int = 4,\n",
    "        stride: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        reduced_features = out_features // reduction\n",
    "        self.block = nn.Sequential(\n",
    "            # wide -> narrow\n",
    "            ConvNormAct(\n",
    "                in_features, reduced_features, kernel_size=1, stride=stride, bias=False\n",
    "            ),\n",
    "            # narrow -> narrow\n",
    "            ConvNormAct(reduced_features, reduced_features, kernel_size=3, bias=False),\n",
    "            # narrow -> wide\n",
    "            ConvNormAct(reduced_features, out_features, kernel_size=1, bias=False, act=nn.Identity),\n",
    "        )\n",
    "        self.shortcut = (\n",
    "            nn.Sequential(\n",
    "                ConvNormAct(\n",
    "                    in_features, out_features, kernel_size=1, stride=stride, bias=False\n",
    "                )\n",
    "            )\n",
    "            if in_features != out_features\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        res = x\n",
    "        x = self.block(x)\n",
    "        res = self.shortcut(res)\n",
    "        x += res\n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 7, 7])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(1, 32, 7, 7)\n",
    "block = BottleNeckBlock(32, 64)\n",
    "block(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define a `Stage`, a collection of `blocks`. Each stage usually downsamples the input by a factor of `2`, this is done in the first block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNexStage(nn.Sequential):\n",
    "    def __init__(\n",
    "        self, in_features: int, out_features: int, depth: int, stride: int = 2, **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            # downsample is done here\n",
    "            BottleNeckBlock(in_features, out_features, stride=stride, **kwargs),\n",
    "            *[\n",
    "                BottleNeckBlock(out_features, out_features, **kwargs)\n",
    "                for _ in range(depth - 1)\n",
    "            ],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 4, 4])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage = ConvNexStage(32, 64, depth=2)\n",
    "stage(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, notice how the input was reduced from `7x7` to `4x4`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet also has what is called `stem`, the first layer in the model that does the heavy downsampling of the input image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNextStem(nn.Sequential):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__(\n",
    "            ConvNormAct(\n",
    "                in_features, out_features, kernel_size=7, stride=2\n",
    "            ),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, now we can define `ConvNextEncoder` that holds a list of stages and takes an image as input producing the final embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNextEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        stem_features: int,\n",
    "        depths: List[int],\n",
    "        widths: List[int],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.stem = ConvNextStem(in_channels, stem_features)\n",
    "\n",
    "        in_out_widths = list(zip(widths, widths[1:]))\n",
    "\n",
    "        self.stages = nn.ModuleList(\n",
    "            [\n",
    "                ConvNexStage(stem_features, widths[0], depths[0], stride=1),\n",
    "                *[\n",
    "                    ConvNexStage(in_features, out_features, depth)\n",
    "                    for (in_features, out_features), depth in zip(\n",
    "                        in_out_widths, depths[1:]\n",
    "                    )\n",
    "                ],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        for stage in self.stages:\n",
    "            x = stage(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 7, 7])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.rand(1, 3, 224, 224)\n",
    "encoder = ConvNextEncoder(in_channels=3, stem_features=64, depths=[3,4,6,4], widths=[256, 512, 1024, 2048])\n",
    "encoder(image).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is your normal `resnet50` encoder, if you attach a classification head you get back the good old resnet50 ready to be train on image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macro Design\n",
    "\n",
    "### Changing stage compute ratio\n",
    "\n",
    "In ResNet we have 4 stages, Swin Transformer uses a ratio of `1:1:3:1` (so one block in the first stage, one in the second, third in the third one ...). Adjusting ResNet50 to this ratio (`(3, 4, 6, 3)` -> `(3, 3, 9, 3)`) results in a performance increase from `78.8%` to `79.4%`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ConvNextEncoder(in_channels=3, stem_features=64, depths=[3,3,9,3], widths=[256, 512, 1024, 2048])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing stem to “Patchify”\n",
    "\n",
    "ResNet stem uses a very aggressive 7x7 conv and a maxpool to heavily downsample the input images. However, Transformers uses a \"patchify\" stem, meaning they embed the input images in patches. Vision Transfomers uses very aggressive patching (16x16), the authors use 4x4 patch implemented with conv layer. The accuracy changes from `79.4%` to `79.5%` suggesting patching works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNextStem(nn.Sequential):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(in_features, out_features, kernel_size=4, stride=4),\n",
    "            nn.BatchNorm2d(out_features)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNeXt-ify\n",
    "\n",
    "[ResNetXt](https://arxiv.org/abs/1512.03385) employs grouped convolution for the 3x3 conv layer in the BottleNeck to reduce FLOPS. In ConvNext, they use depth-wise convolution (like in MobileNet and later in EfficientNet). Depth-wise convs are grouped convolutions where the number of groups is equal to the number of input channels. \n",
    "\n",
    "The authors notice that is very similar to the weighted sum operation in self-attention, which mixes information only in the spatial dimension. Using depth-wise convs reduce the accuracy (since we are not increasing the widths like in ResNetXt), this is expected. \n",
    "\n",
    "So we change our 3x3 conv inside `BottleNeck` block to \n",
    "\n",
    "```\n",
    "ConvNormAct(reduced_features, reduced_features, kernel_size=3, bias=False, groups=reduced_features)\n",
    "```\n",
    "\n",
    "### Inverted Bottleneck\n",
    "\n",
    "Our BottleNeck first reduces the features via a 1x1 conv, then it applies the heavy 3x3 conv and finally expands the features to the original size. An inverted bottleneck block, does the opposite. I have a [whole article](https://medium.com/p/89d7b7e7c6bc) with nice visualization about them.\n",
    "\n",
    "So we go from `wide -> narrow -> wide` to `narrow -> wide -> narrow`. \n",
    "\n",
    "This is similar to Transformers, since the MLP layer follows the `narrow -> wide -> narrow` design, the second dense layer in the MLP expands the input's feature by a factor of four."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNeckBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        expansion: int = 4,\n",
    "        stride: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        expanded_features = out_features * expansion\n",
    "        self.block = nn.Sequential(\n",
    "            # narrow -> wide\n",
    "            ConvNormAct(\n",
    "                in_features, expanded_features, kernel_size=1, stride=stride, bias=False\n",
    "            ),\n",
    "            # wide -> wide (with depth-wise)\n",
    "            ConvNormAct(expanded_features, expanded_features, kernel_size=3, bias=False, groups=in_features),\n",
    "            # wide -> narrow\n",
    "            ConvNormAct(expanded_features, out_features, kernel_size=1, bias=False, act=nn.Identity),\n",
    "        )\n",
    "        self.shortcut = (\n",
    "            nn.Sequential(\n",
    "                ConvNormAct(\n",
    "                    in_features, out_features, kernel_size=1, stride=stride, bias=False\n",
    "                )\n",
    "            )\n",
    "            if in_features != out_features\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        res = x\n",
    "        x = self.block(x)\n",
    "        res = self.shortcut(res)\n",
    "        x += res\n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Kernel Sizes\n",
    "\n",
    "Modern Vision Transfomer, like Swin, uses a bigger kernel size (7x7). Increasing the kernel size will make the computation more expensive, so we move up the big depth-wise conv, by doing so we will have fewer channels. The authors note this is similar to Transformers model where the Multihead Self Attention (MSA) is done before the MLP layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNeckBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        expansion: int = 4,\n",
    "        stride: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        expanded_features = out_features * expansion\n",
    "        self.block = nn.Sequential(\n",
    "            # narrow -> wide (with depth-wise and bigger kernel)\n",
    "            ConvNormAct(\n",
    "                in_features, in_features, kernel_size=7, stride=stride, bias=False, groups=in_features\n",
    "            ),\n",
    "            # wide -> wide \n",
    "            ConvNormAct(in_features, expanded_features, kernel_size=1),\n",
    "            # wide -> narrow\n",
    "            ConvNormAct(expanded_features, out_features, kernel_size=1, bias=False, act=nn.Identity),\n",
    "        )\n",
    "        self.shortcut = (\n",
    "            nn.Sequential(\n",
    "                ConvNormAct(\n",
    "                    in_features, out_features, kernel_size=1, stride=stride, bias=False\n",
    "                )\n",
    "            )\n",
    "            if in_features != out_features\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        res = x\n",
    "        x = self.block(x)\n",
    "        res = self.shortcut(res)\n",
    "        x += res\n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This increases accuracy from `79.9%` to `80.6%`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Micro Design\n",
    "### Replacing ReLU with GELU\n",
    "\n",
    "Since GELU is used by the most advanced transformers, why not use it in our model? The authors report the accuracy stays unchanged. In PyTorch GELU in `nn.GELU`.\n",
    "\n",
    "### Fewer activation functions\n",
    "\n",
    "Our block has three activation functions. While, in Transformer block, there is only one activation function, the one inside the MLP block. The authors removed all the activations except for the one after the middle conv layer. This improves accuracy to `81.3%` matching Swin-T!\n",
    "\n",
    "\n",
    "### Fewer normalization layers\n",
    "\n",
    "Similar to activations, Transformers blocks have fewer normalization layers. The authors decide the remove all the BatchNorm and kept only the one before the middle conv.\n",
    "\n",
    "### Substituting BN with LN\n",
    "\n",
    "Well, they substitute the BatchNorm layers with LinearyNorm. They note that doing so in the original ResNet hurts performance, but after all our changes, the performance increases to `81.5%`\n",
    "\n",
    "So, let's apply them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNeckBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        expansion: int = 4,\n",
    "        stride: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        expanded_features = out_features * expansion\n",
    "        self.block = nn.Sequential(\n",
    "            # narrow -> wide (with depth-wise and bigger kernel)\n",
    "            nn.Conv2d(\n",
    "                in_features, in_features, kernel_size=7, stride=stride, bias=False, groups=in_features\n",
    "            ),\n",
    "            # GroupNorm with num_groups=1 is the same as LayerNorm but works for 2D data\n",
    "            nn.GroupNorm(num_groups=1, num_channels=in_features),\n",
    "            # wide -> wide \n",
    "            nn.Conv2d(in_features, expanded_features, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            # wide -> narrow\n",
    "            nn.Conv2d(expanded_features, out_features, kernel_size=1),\n",
    "        )\n",
    "        self.shortcut = (\n",
    "            nn.Sequential(\n",
    "                ConvNormAct(\n",
    "                    in_features, out_features, kernel_size=1, stride=stride, bias=False\n",
    "                )\n",
    "            )\n",
    "            if in_features != out_features\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        res = x\n",
    "        x = self.block(x)\n",
    "        res = self.shortcut(res)\n",
    "        x += res\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate downsampling layers.\n",
    "\n",
    "In ResNet the downsampling is done by the `stride=2` conv. Transformers (and other conv nets too) have a separate downsampling block. The authors removed the `stride=2` and add a downsampling block before the three convs using a `2x2` `stride=2` conv. Normalization is needed before the downsampling operation to maintain stability during training. We can add this module to our `ConvNexStage`. Finally, we reach `82.0%` surpassing Swin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNexStage(nn.Sequential):\n",
    "    def __init__(\n",
    "        self, in_features: int, out_features: int, depth: int, **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            # add the downsampler\n",
    "            nn.Sequential(\n",
    "                nn.GroupNorm(num_groups=1, num_channels=in_features),\n",
    "                nn.Conv2d(in_features, out_features, kernel_size=2, stride=2)\n",
    "            ),\n",
    "            *[\n",
    "                BottleNeckBlock(out_features, out_features, **kwargs)\n",
    "                for _ in range(depth)\n",
    "            ],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can clean our `BottleNeckBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNeckBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        expansion: int = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        expanded_features = out_features * expansion\n",
    "        self.block = nn.Sequential(\n",
    "            # narrow -> wide (with depth-wise and bigger kernel)\n",
    "            nn.Conv2d(\n",
    "                in_features, in_features, kernel_size=7, padding=3, bias=False, groups=in_features\n",
    "            ),\n",
    "            # GroupNorm with num_groups=1 is the same as LayerNorm but works for 2D data\n",
    "            nn.GroupNorm(num_groups=1, num_channels=in_features),\n",
    "            # wide -> wide \n",
    "            nn.Conv2d(in_features, expanded_features, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            # wide -> narrow\n",
    "            nn.Conv2d(expanded_features, out_features, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        res = x\n",
    "        x = self.block(x)\n",
    "        x += res\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally arrive to our final block! Let's test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 62, 7, 7])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage = ConvNexStage(32, 62, depth=1)\n",
    "stage(torch.randn(1, 32, 14, 14)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final touches\n",
    "\n",
    "They also added Stochastic Depth, also known as Drop Path, (I have an [article](https://towardsdatascience.com/implementing-stochastic-depth-drop-path-in-pytorch-291498c4a974) about it) and Layer Scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import StochasticDepth\n",
    "\n",
    "class LayerScaler(nn.Module):\n",
    "    def __init__(self, init_value: float, dimensions: int):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(init_value * torch.ones((dimensions)), \n",
    "                                    requires_grad=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.gamma[None,...,None,None] * x\n",
    "\n",
    "class BottleNeckBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        expansion: int = 4,\n",
    "        drop_p: float = .0,\n",
    "        layer_scaler_init_value: float = 1e-6,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        expanded_features = out_features * expansion\n",
    "        self.block = nn.Sequential(\n",
    "            # narrow -> wide (with depth-wise and bigger kernel)\n",
    "            nn.Conv2d(\n",
    "                in_features, in_features, kernel_size=7, padding=3, bias=False, groups=in_features\n",
    "            ),\n",
    "            # GroupNorm with num_groups=1 is the same as LayerNorm but works for 2D data\n",
    "            nn.GroupNorm(num_groups=1, num_channels=in_features),\n",
    "            # wide -> wide \n",
    "            nn.Conv2d(in_features, expanded_features, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            # wide -> narrow\n",
    "            nn.Conv2d(expanded_features, out_features, kernel_size=1),\n",
    "        )\n",
    "        self.layer_scaler = LayerScaler(layer_scaler_init_value, out_features)\n",
    "        self.drop_path = StochasticDepth(drop_p, mode=\"batch\")\n",
    "\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        res = x\n",
    "        x = self.block(x)\n",
    "        x = self.layer_scaler(x)\n",
    "        x = self.drop_path(x)\n",
    "        x += res\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "et voilà 🎉 We have arrived to the final ConvNext Block! Let's see if it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 62, 7, 7])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage = ConvNexStage(32, 62, depth=1)\n",
    "stage(torch.randn(1, 32, 14, 14)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super! We need to create the drop paths probabilities in the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNextEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        stem_features: int,\n",
    "        depths: List[int],\n",
    "        widths: List[int],\n",
    "        drop_p: float = .0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.stem = ConvNextStem(in_channels, stem_features)\n",
    "\n",
    "        in_out_widths = list(zip(widths, widths[1:]))\n",
    "        # create drop paths probabilities (one for each stage)\n",
    "        drop_probs = [x.item() for x in torch.linspace(0, drop_p, sum(depths))] \n",
    "        \n",
    "        self.stages = nn.ModuleList(\n",
    "            [\n",
    "                ConvNexStage(stem_features, widths[0], depths[0], drop_p=drop_probs[0]),\n",
    "                *[\n",
    "                    ConvNexStage(in_features, out_features, depth, drop_p=drop_p)\n",
    "                    for (in_features, out_features), depth, drop_p in zip(\n",
    "                        in_out_widths, depths[1:], drop_probs[1:]\n",
    "                    )\n",
    "                ],\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        for stage in self.stages:\n",
    "            x = stage(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 3, 3])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.rand(1, 3, 224, 224)\n",
    "encoder = ConvNextEncoder(in_channels=3, stem_features=64, depths=[3,4,6,4], widths=[256, 512, 1024, 2048])\n",
    "encoder(image).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the final ConvNext used for Image classification we need to apply a classification head on top of the encoder. We also add a `LayerNorm` before the last linear layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, num_channels: int, num_classes: int = 1000):\n",
    "        super().__init__(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(1),\n",
    "            nn.LayerNorm(num_channels),\n",
    "            nn.Linear(num_channels, num_classes)\n",
    "        )\n",
    "    \n",
    "    \n",
    "class ConvNextForImageClassification(nn.Sequential):\n",
    "    def __init__(self,  \n",
    "                 in_channels: int,\n",
    "                 stem_features: int,\n",
    "                 depths: List[int],\n",
    "                 widths: List[int],\n",
    "                 drop_p: float = .0,\n",
    "                 num_classes: int = 1000):\n",
    "        super().__init__()\n",
    "        self.encoder = ConvNextEncoder(in_channels, stem_features, depths, widths, drop_p)\n",
    "        self.head = ClassificationHead(widths[-1], num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.rand(1, 3, 224, 224)\n",
    "classifier = ConvNextForImageClassification(in_channels=3, stem_features=64, depths=[3,4,6,4], widths=[256, 512, 1024, 2048])\n",
    "classifier(image).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here you have it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In this article we have seen, step by step, all the changes the Authors did to create ConvNext from ResNet. I hope this was useful :)\n",
    "\n",
    "Thank you for reading it!\n",
    "\n",
    "Francesco"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b068f65b37070278da3139a5b2cbbfa33e93d7683ce6ce1a4914b8bf50338e31"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
